{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47595919",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {'ISBN': 'str', 'Book-Title': 'str', 'Book-Author': 'str', 'Year-Of-Publication': 'str', 'Publisher': 'str', 'Image-URL-S': 'str', 'Image-URL-M': 'str', 'Image-URL-L': 'str'}\n",
    "\n",
    "# books_df = pd.read_csv('dataset/Books.csv', dtype=dtypes)\n",
    "# users_df = pd.read_csv('dataset/Users.csv')\n",
    "ratings_df = pd.read_csv('dataset/Ratings.csv')\n",
    "# movie_ratings_df = pd.read_csv('dataset/MovieRatings.csv')\n",
    "ratings_df['Book-Rating'] = ratings_df['Book-Rating'] / 2\n",
    "ratings_df.to_csv('dataset/ratings2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4722902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ratings_df.merge(books_df, how=\"left\", on=\"ISBN\")\n",
    "# df.head().to_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2953c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.info()\n",
    "ratings_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask that is True for rows that don't have a Book-Rating of 0\n",
    "mask = ratings_df['Book-Rating'] != 0\n",
    "\n",
    "# Use boolean indexing to select only the rows that don't have a Book-Rating of 0\n",
    "ratings_df = ratings_df[mask]\n",
    "ratings_df.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eadb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_book = preprocessing.LabelEncoder()\n",
    "ratings_df['User-ID'] = lbl_user.fit_transform(ratings_df['User-ID'].values)\n",
    "ratings_df['ISBN'] = lbl_book.fit_transform(ratings_df['ISBN'].values)\n",
    "\n",
    "user_ratings_count = Counter(ratings_df['User-ID'])\n",
    "\n",
    "# Find users with less than 4 ratings\n",
    "users_to_remove = [user_id for user_id, count in user_ratings_count.items() if count < 10]\n",
    "\n",
    "# Remove users with less than 4 ratings from the dataset\n",
    "ratings_df = ratings_df[~ratings_df['User-ID'].isin(users_to_remove)]\n",
    "ratings_df.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d73294",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    ratings_df, test_size=0.1, stratify=ratings_df['Book-Rating'].values\n",
    ")\n",
    "\n",
    "# train_df.to_csv('dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookDataset import BookDataset\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = BookDataset(train_df['User-ID'].values, train_df['ISBN'].values, train_df['Book-Rating'].values)\n",
    "valid_dataset = BookDataset(valid_df['User-ID'].values, valid_df['ISBN'].values, valid_df['Book-Rating'].values)\n",
    "\n",
    "# Create train and validation data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookRecommender(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_isbns, embedding_dim):\n",
    "        super(BookRecommender, self).__init__()\n",
    "        self.user_embedding = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_dim)\n",
    "        self.isbn_embedding = torch.nn.Embedding(num_embeddings=num_isbns, embedding_dim=embedding_dim)\n",
    "        self.fc1 = torch.nn.Linear(embedding_dim * 2, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, users, isbns):\n",
    "        user_embeds = self.user_embedding(users.long())\n",
    "        isbn_embeds = self.isbn_embedding(isbns.long())\n",
    "        embeds = torch.cat([user_embeds, isbn_embeds], dim=1)\n",
    "        x = torch.relu(self.fc1(embeds.view(embeds.size(0), -1)))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622eca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BookRecommender(num_users=len(lbl_user.classes_),\n",
    "                        num_isbns=len(lbl_book.classes_),\n",
    "                        embedding_dim=64)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5cdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler= optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "num_batches = len(train_loader)\n",
    "losses = []  # List to store the loss values\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch[\"user_id\"], batch[\"isbn\"])\n",
    "        loss = criterion(outputs, batch[\"rating\"].unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / num_batches\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, epoch_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "total_loss = 0.0\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "print('hello', batch_size)\n",
    "predictions = []\n",
    "targets = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        user_ids, isbns, ratings = batch['user_id'], batch['isbn'], batch['rating']\n",
    "        outputs = model(user_ids, isbns)\n",
    "        ratings = ratings.view(-1, 1)  # Reshape the target tensor\n",
    "        predicted_ratings = torch.round(outputs)  # Round the predicted ratings\n",
    "#         print(ratings.shape)\n",
    "        predictions.extend(predicted_ratings.tolist())\n",
    "        targets.extend(ratings.tolist())\n",
    "    \n",
    "        correct = ((predicted_ratings == ratings) | (predicted_ratings == ratings - 0.5) | (predicted_ratings == ratings + 0.5)).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += ratings.size(0)\n",
    "\n",
    "# Calculate mean squared error\n",
    "rmse = mean_squared_error(targets, predictions)\n",
    "print('Validation RMSE: {:.2f}'.format(rmse))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print('Validation Accuracy: {:.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aea322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookDataset import BookDataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 10,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 100,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.SmoothL1Loss(),\n",
    "        'num_epochs': 20,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.SmoothL1Loss(),\n",
    "        'num_epochs': 200,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.001,\n",
    "        'loss_function': torch.nn.L1Loss(),\n",
    "        'num_epochs': 5,\n",
    "        'scheduler': optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9),\n",
    "        'num_batches': 32\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.001,\n",
    "        'loss_function': torch.nn.CrossEntropyLoss(),\n",
    "        'num_epochs': 15,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 256\n",
    "    }\n",
    "]\n",
    "\n",
    "# Open the output file in append mode\n",
    "with open(\"output.txt\", \"a\") as output_file:\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        learning_rate = scenario['learning_rate']\n",
    "        loss_function = scenario['loss_function']\n",
    "        num_epochs = scenario['num_epochs']\n",
    "        scheduler = scenario['scheduler']\n",
    "        num_batches = scenario['num_batches']\n",
    "        \n",
    "        # Create a new instance of the model and optimizer with the current hyperparameters\n",
    "        model = BookRecommender(num_users=len(lbl_user.classes_), num_isbns=len(lbl_book.classes_), embedding_dim=64)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Create train and validation data loaders based on number of batches\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        \n",
    "        # Print the current scenario and number of batches\n",
    "        output_file.write(f\"Scenario {i+1}: Learning Rate = {learning_rate}, Loss Function = {loss_function.__class__.__name__}\\n\")\n",
    "        output_file.write(f\"Number of Batches: {num_batches}\\n\")\n",
    "        if scheduler is not None:\n",
    "            output_file.write(f\"Scheduler: {scheduler.__class__.__name__}\\n\")\n",
    "        output_file.write(\"Epoch\\tLoss\\tValidation RMSE\\tValidation Accuracy\\n\")\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []  # List to store the training loss values\n",
    "        valid_losses = []  # List to store the validation RMSE values\n",
    "        valid_accuracies = []  # List to store the validation accuracies\n",
    "        rmse_values = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Switch to training mode\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch[\"user_id\"], batch[\"isbn\"])\n",
    "                loss = loss_function(outputs, batch[\"rating\"].unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "\n",
    "            # Perform validation\n",
    "            model.eval()  # Switch to evaluation mode\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    user_ids, isbns, ratings = batch['user_id'], batch['isbn'], batch['rating']\n",
    "                    outputs = model(user_ids, isbns)\n",
    "                    ratings = ratings.view(-1, 1)  # Reshape the target tensor\n",
    "                    predicted_ratings = torch.round(outputs)  # Round the predicted ratings\n",
    "                    predictions.extend(predicted_ratings.tolist())\n",
    "                    targets.extend(ratings.tolist())\n",
    "                    correct = ((predicted_ratings == ratings) | (predicted_ratings == ratings - 0.5) | (predicted_ratings == ratings + 0.5)).sum().item()\n",
    "                    total_correct += correct\n",
    "                    total_samples += ratings.size(0)\n",
    "                    loss = loss_function(outputs, ratings)\n",
    "                    total_loss += loss.item()\n",
    "            valid_loss = total_loss / len(valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = mean_squared_error(targets, predictions, squared=False)\n",
    "            rmse_values.append(rmse)\n",
    "            # Calculate accuracy\n",
    "            accuracy = total_correct / total_samples\n",
    "\n",
    "            valid_accuracies.append(accuracy)\n",
    "\n",
    "            # Save the epoch results to the output file\n",
    "            output_file.write(f\"{epoch+1}\\t{epoch_loss:.4f}\\t{valid_loss:.4f}\\t{rmse:.2f}\\t{accuracy:.2f}\\n\")\n",
    "\n",
    "            # Adjust the learning rate using the scheduler if provided\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        output_file.write(\"\\n\")  # Add a separator between different scenarios\n",
    "\n",
    "        # Plot the training and validation losses\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(valid_losses, label='Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss / RMSE')\n",
    "        plt.title(f'Loss and RMSE - Scenario {i+1}')\n",
    "        plt.legend()\n",
    "        plot_file = os.path.join(\"plots\", f'scenario_{i+1}_loss_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        # Plot the validation accuracy\n",
    "        plt.plot(valid_accuracies)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Validation Accuracy - Scenario {i+1}')\n",
    "        plot_file = os.path.join(\"plots\", f'scenario_{i+1}_accuracy_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2198ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37cbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404ea82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
