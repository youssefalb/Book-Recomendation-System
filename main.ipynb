{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78c8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47595919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   User-ID      1149780 non-null  int64  \n",
      " 1   ISBN         1149780 non-null  object \n",
      " 2   Book-Rating  1149780 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 26.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dtypes = {'ISBN': 'str', 'Book-Title': 'str', 'Book-Author': 'str', 'Year-Of-Publication': 'str', 'Publisher': 'str', 'Image-URL-S': 'str', 'Image-URL-M': 'str', 'Image-URL-L': 'str'}\n",
    "\n",
    "# books_df = pd.read_csv('dataset/Books.csv', dtype=dtypes)\n",
    "# users_df = pd.read_csv('dataset/Users.csv')\n",
    "ratings_df = pd.read_csv('dataset/Ratings.csv')\n",
    "# movie_ratings_df = pd.read_csv('dataset/MovieRatings.csv')\n",
    "ratings_df['Book-Rating'] = ratings_df['Book-Rating'] / 2\n",
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4722902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ratings_df.merge(books_df, how=\"left\", on=\"ISBN\")\n",
    "# df.head().to_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2953c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   User-ID      1149780 non-null  int64  \n",
      " 1   ISBN         1149780 non-null  object \n",
      " 2   Book-Rating  1149780 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 26.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          User-ID         ISBN  Book-Rating\n",
       "0         276725   034545104X          0.0\n",
       "1         276726   0155061224          2.5\n",
       "2         276727   0446520802          0.0\n",
       "3         276729   052165615X          1.5\n",
       "4         276729   0521795028          3.0\n",
       "...          ...          ...          ...\n",
       "1149775   276704   1563526298          4.5\n",
       "1149776   276706   0679447156          0.0\n",
       "1149777   276709   0515107662          5.0\n",
       "1149778   276721   0590442449          5.0\n",
       "1149779   276723  05162443314          4.0\n",
       "\n",
       "[1149780 rows x 3 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.info()\n",
    "ratings_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd7fc2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          User-ID         ISBN  Book-Rating\n",
       "1         276726   0155061224          2.5\n",
       "3         276729   052165615X          1.5\n",
       "4         276729   0521795028          3.0\n",
       "6         276736   3257224281          4.0\n",
       "7         276737   0600570967          3.0\n",
       "...          ...          ...          ...\n",
       "1149773   276704   0806917695          2.5\n",
       "1149775   276704   1563526298          4.5\n",
       "1149777   276709   0515107662          5.0\n",
       "1149778   276721   0590442449          5.0\n",
       "1149779   276723  05162443314          4.0\n",
       "\n",
       "[433671 rows x 3 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a boolean mask that is True for rows that don't have a Book-Rating of 0\n",
    "mask = ratings_df['Book-Rating'] != 0\n",
    "\n",
    "# Use boolean indexing to select only the rows that don't have a Book-Rating of 0\n",
    "ratings_df = ratings_df[mask]\n",
    "ratings_df.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66eadb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youssef albali\\AppData\\Local\\Temp\\ipykernel_28356\\2559493324.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_df['User-ID'] = lbl_user.fit_transform(ratings_df['User-ID'].values)\n",
      "C:\\Users\\youssef albali\\AppData\\Local\\Temp\\ipykernel_28356\\2559493324.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_df['ISBN'] = lbl_book.fit_transform(ratings_df['ISBN'].values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          User-ID    ISBN  Book-Rating\n",
       "133        77209    2884          5.0\n",
       "134        77209   15804          4.5\n",
       "135        77209   16137          5.0\n",
       "136        77209   17027          4.5\n",
       "137        77209   39925          4.5\n",
       "...          ...     ...          ...\n",
       "1149743    77175  117346          5.0\n",
       "1149744    77175  117457          5.0\n",
       "1149745    77175  125788          5.0\n",
       "1149746    77175  134545          3.0\n",
       "1149747    77175  141488          3.5\n",
       "\n",
       "[295561 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_book = preprocessing.LabelEncoder()\n",
    "ratings_df['User-ID'] = lbl_user.fit_transform(ratings_df['User-ID'].values)\n",
    "ratings_df['ISBN'] = lbl_book.fit_transform(ratings_df['ISBN'].values)\n",
    "\n",
    "user_ratings_count = Counter(ratings_df['User-ID'])\n",
    "\n",
    "# Find users with less than 10 ratings\n",
    "users_to_remove = [user_id for user_id, count in user_ratings_count.items() if count < 10]\n",
    "\n",
    "# Remove users with less than 10 ratings from the dataset\n",
    "ratings_df = ratings_df[~ratings_df['User-ID'].isin(users_to_remove)]\n",
    "ratings_df.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d73294",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    ratings_df, test_size=0.1, stratify=ratings_df['Book-Rating'].values\n",
    ")\n",
    "\n",
    "# train_df.to_csv('dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d26b448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29557, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebd6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookDataset import BookDataset\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = BookDataset(train_df['User-ID'].values, train_df['ISBN'].values, train_df['Book-Rating'].values)\n",
    "valid_dataset = BookDataset(valid_df['User-ID'].values, valid_df['ISBN'].values, valid_df['Book-Rating'].values)\n",
    "\n",
    "# Create train and validation data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ff1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookRecommender(torch.nn.Module):\n",
    "    def __init__(self, num_users, num_isbns, embedding_dim):\n",
    "        super(BookRecommender, self).__init__()\n",
    "        self.user_embedding = torch.nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_dim)\n",
    "        self.isbn_embedding = torch.nn.Embedding(num_embeddings=num_isbns, embedding_dim=embedding_dim)\n",
    "        self.fc1 = torch.nn.Linear(embedding_dim * 2, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, users, isbns):\n",
    "        user_embeds = self.user_embedding(users.long())\n",
    "        isbn_embeds = self.isbn_embedding(isbns.long())\n",
    "        embeds = torch.cat([user_embeds, isbn_embeds], dim=1)\n",
    "        x = torch.relu(self.fc1(embeds.view(embeds.size(0), -1)))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622eca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BookRecommender(\n",
      "  (user_embedding): Embedding(77805, 64)\n",
      "  (isbn_embedding): Embedding(185973, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BookRecommender(num_users=len(lbl_user.classes_),\n",
    "                        num_isbns=len(lbl_book.classes_),\n",
    "                        embedding_dim=64)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5cdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler= optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b7f1e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# num_batches = len(train_loader)\n",
    "# losses = []  # List to store the loss values\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch[\"user_id\"], batch[\"isbn\"])\n",
    "#         loss = criterion(outputs, batch[\"rating\"].unsqueeze(1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "    \n",
    "#     epoch_loss = running_loss / num_batches\n",
    "#     losses.append(epoch_loss)\n",
    "    \n",
    "#     print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, epoch_loss))\n",
    "#     scheduler.step()\n",
    "\n",
    "# # Plot the training loss\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b39ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# total_loss = 0.0\n",
    "# total_correct = 0\n",
    "# total_samples = 0\n",
    "# print('hello', batch_size)\n",
    "# predictions = []\n",
    "# targets = []\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in valid_loader:\n",
    "#         user_ids, isbns, ratings = batch['user_id'], batch['isbn'], batch['rating']\n",
    "#         outputs = model(user_ids, isbns)\n",
    "#         ratings = ratings.view(-1, 1)  # Reshape the target tensor\n",
    "#         predicted_ratings = torch.round(outputs)  # Round the predicted ratings\n",
    "# #         print(ratings.shape)\n",
    "#         predictions.extend(predicted_ratings.tolist())\n",
    "#         targets.extend(ratings.tolist())\n",
    "    \n",
    "#         correct = ((predicted_ratings == ratings) | (predicted_ratings == ratings - 0.5) | (predicted_ratings == ratings + 0.5)).sum().item()\n",
    "#         total_correct += correct\n",
    "#         total_samples += ratings.size(0)\n",
    "\n",
    "# # Calculate mean squared error\n",
    "# rmse = mean_squared_error(targets, predictions)\n",
    "# print('Validation RMSE: {:.2f}'.format(rmse))\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = total_correct / total_samples\n",
    "# print('Validation Accuracy: {:.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3aea322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "C:\\Users\\Public\\Anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from bookDataset import BookDataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "os.makedirs(\"plots_2\", exist_ok=True)\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 10,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 20,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 32\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 100,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.SmoothL1Loss(),\n",
    "        'num_epochs': 20,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    },\n",
    "    {\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 200,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    }\n",
    "]\n",
    "\n",
    "# Open the output file in append mode\n",
    "with open(\"output_2.txt\", \"a\") as output_file:\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        learning_rate = scenario['learning_rate']\n",
    "        loss_function = scenario['loss_function']\n",
    "        num_epochs = scenario['num_epochs']\n",
    "        scheduler = scenario['scheduler']\n",
    "        num_batches = scenario['num_batches']\n",
    "        \n",
    "        # Create a new instance of the model and optimizer with the current hyperparameters\n",
    "        model = BookRecommender(num_users=len(lbl_user.classes_), num_isbns=len(lbl_book.classes_), embedding_dim=64)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Create train and validation data loaders based on number of batches\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        \n",
    "        # Print the current scenario and number of batches\n",
    "        output_file.write(f\"Scenario {i+1}: Learning Rate = {learning_rate}, Loss Function = {loss_function.__class__.__name__}\\n\")\n",
    "        output_file.write(f\"Number of Batches: {num_batches}\\n\")\n",
    "        if scheduler is not None:\n",
    "            output_file.write(f\"Scheduler: {scheduler.__class__.__name__}\\n\")\n",
    "        output_file.write(\"Epoch\\tLoss\\tValidation loss\\tRMSE\\tValidation Accuracy\\n\")\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []  # List to store the training loss values\n",
    "        valid_losses = []  # List to store the validation RMSE values\n",
    "        valid_accuracies = []  # List to store the validation accuracies\n",
    "        rmse_values = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Switch to training mode\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch[\"user_id\"], batch[\"isbn\"])\n",
    "                loss = loss_function(outputs, batch[\"rating\"].unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "\n",
    "            # Perform validation\n",
    "            model.eval()  # Switch to evaluation mode\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    user_ids, isbns, ratings = batch['user_id'], batch['isbn'], batch['rating']\n",
    "                    outputs = model(user_ids, isbns)\n",
    "                    ratings = ratings.view(-1, 1)  # Reshape the target tensor\n",
    "                    predicted_ratings = torch.round(outputs)  # Round the predicted ratings\n",
    "                    predictions.extend(predicted_ratings.tolist())\n",
    "                    targets.extend(ratings.tolist())\n",
    "                    correct = ((predicted_ratings == ratings) | (predicted_ratings == ratings - 0.5) | (predicted_ratings == ratings + 0.5)).sum().item()\n",
    "                    total_correct += correct\n",
    "                    total_samples += ratings.size(0)\n",
    "                    loss = loss_function(outputs, ratings)\n",
    "                    total_loss += loss.item()\n",
    "            valid_loss = total_loss / len(valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = mean_squared_error(targets, predictions, squared=True)\n",
    "            rmse_values.append(rmse)\n",
    "            # Calculate accuracy\n",
    "            accuracy = total_correct / total_samples\n",
    "\n",
    "            valid_accuracies.append(accuracy)\n",
    "\n",
    "            # Save the epoch results to the output file\n",
    "            output_file.write(f\"{epoch+1}\\t{epoch_loss:.4f}\\t{valid_loss:.4f}\\t{rmse:.2f}\\t{100*accuracy:.2f}\\n\")\n",
    "\n",
    "            # Adjust the learning rate using the scheduler if provided\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        output_file.write(\"\\n\")  # Add a separator between different scenarios\n",
    "\n",
    "        # Plot the training and validation losses\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(valid_losses, label='Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss / RMSE')\n",
    "        plt.title(f'Loss and RMSE - Scenario {i+1}')\n",
    "        plt.legend()\n",
    "        plot_file = os.path.join(\"plots_2\", f'scenario_{i+1}_loss_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        # Plot the validation accuracy\n",
    "        plt.plot(valid_accuracies)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Validation Accuracy - Scenario {i+1}')\n",
    "        plot_file = os.path.join(\"plots_2\", f'scenario_{i+1}_accuracy_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2198ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37cbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404ea82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2ea4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
