{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78c8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model import Recommender\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from dataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47595919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   User-ID      1149780 non-null  int64  \n",
      " 1   ISBN         1149780 non-null  object \n",
      " 2   Book-Rating  1149780 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 26.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dtypes = {'ISBN': 'str', 'Book-Title': 'str', 'Book-Author': 'str', 'Year-Of-Publication': 'str', 'Publisher': 'str', 'Image-URL-S': 'str', 'Image-URL-M': 'str', 'Image-URL-L': 'str'}\n",
    "\n",
    "# books_df = pd.read_csv('dataset/Books.csv', dtype=dtypes)\n",
    "# users_df = pd.read_csv('dataset/Users.csv')\n",
    "ratings_df = pd.read_csv('dataset/Ratings.csv')\n",
    "# movie_ratings_df = pd.read_csv('dataset/MovieRatings.csv')\n",
    "ratings_df['Book-Rating'] = ratings_df['Book-Rating'] / 2\n",
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4722902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ratings_df.merge(books_df, how=\"left\", on=\"ISBN\")\n",
    "# df.head().to_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2953c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1149780 entries, 0 to 1149779\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   User-ID      1149780 non-null  int64  \n",
      " 1   ISBN         1149780 non-null  object \n",
      " 2   Book-Rating  1149780 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 26.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          User-ID         ISBN  Book-Rating\n",
       "0         276725   034545104X          0.0\n",
       "1         276726   0155061224          2.5\n",
       "2         276727   0446520802          0.0\n",
       "3         276729   052165615X          1.5\n",
       "4         276729   0521795028          3.0\n",
       "...          ...          ...          ...\n",
       "1149775   276704   1563526298          4.5\n",
       "1149776   276706   0679447156          0.0\n",
       "1149777   276709   0515107662          5.0\n",
       "1149778   276721   0590442449          5.0\n",
       "1149779   276723  05162443314          4.0\n",
       "\n",
       "[1149780 rows x 3 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.info()\n",
    "ratings_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd7fc2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          User-ID         ISBN  Book-Rating\n",
       "1         276726   0155061224          2.5\n",
       "3         276729   052165615X          1.5\n",
       "4         276729   0521795028          3.0\n",
       "6         276736   3257224281          4.0\n",
       "7         276737   0600570967          3.0\n",
       "...          ...          ...          ...\n",
       "1149773   276704   0806917695          2.5\n",
       "1149775   276704   1563526298          4.5\n",
       "1149777   276709   0515107662          5.0\n",
       "1149778   276721   0590442449          5.0\n",
       "1149779   276723  05162443314          4.0\n",
       "\n",
       "[433671 rows x 3 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a boolean mask that is True for rows that don't have a Book-Rating of 0\n",
    "mask = ratings_df['Book-Rating'] != 0\n",
    "\n",
    "# Use boolean indexing to select only the rows that don't have a Book-Rating of 0\n",
    "ratings_df = ratings_df[mask]\n",
    "ratings_df.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66eadb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\youssef albali\\AppData\\Local\\Temp\\ipykernel_17460\\3737128181.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_df['User-ID'] = lbl_user.fit_transform(ratings_df['User-ID'].values)\n",
      "C:\\Users\\youssef albali\\AppData\\Local\\Temp\\ipykernel_17460\\3737128181.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings_df['ISBN'] = lbl_book.fit_transform(ratings_df['ISBN'].values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          User-ID    ISBN  Book-Rating\n",
       "133        77209    2884          5.0\n",
       "134        77209   15804          4.5\n",
       "135        77209   16137          5.0\n",
       "136        77209   17027          4.5\n",
       "137        77209   39925          4.5\n",
       "...          ...     ...          ...\n",
       "1149743    77175  117346          5.0\n",
       "1149744    77175  117457          5.0\n",
       "1149745    77175  125788          5.0\n",
       "1149746    77175  134545          3.0\n",
       "1149747    77175  141488          3.5\n",
       "\n",
       "[295561 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_book = preprocessing.LabelEncoder()\n",
    "ratings_df['User-ID'] = lbl_user.fit_transform(ratings_df['User-ID'].values)\n",
    "ratings_df['ISBN'] = lbl_book.fit_transform(ratings_df['ISBN'].values)\n",
    "\n",
    "user_ratings_count = Counter(ratings_df['User-ID'])\n",
    "\n",
    "# Find users with less than 4 ratings\n",
    "users_to_remove = [user_id for user_id, count in user_ratings_count.items() if count < 10]\n",
    "\n",
    "# Remove users with less than 10 ratings from the dataset\n",
    "ratings_df = ratings_df[~ratings_df['User-ID'].isin(users_to_remove)]\n",
    "ratings_df.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d73294",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    ratings_df, test_size=0.1, stratify=ratings_df['Book-Rating'].values\n",
    ")\n",
    "\n",
    "# train_df.to_csv('dataset/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d26b448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29557, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebd6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = MyDataset(train_df['User-ID'].values, train_df['ISBN'].values, train_df['Book-Rating'].values)\n",
    "valid_dataset = MyDataset(valid_df['User-ID'].values, valid_df['ISBN'].values, valid_df['Book-Rating'].values)\n",
    "\n",
    "# Create train and validation data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "622eca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender(\n",
      "  (user_embedding): Embedding(77805, 64)\n",
      "  (isbn_embedding): Embedding(185973, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Recommender(num_users=len(lbl_user.classes_),\n",
    "                        num_isbns=len(lbl_book.classes_),\n",
    "                        embedding_dim=64)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5cdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler= optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aea322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "os.makedirs(\"two_fc_layer/Book_plots\", exist_ok=True)\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        'optimizer': 'SGD',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 10,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 10,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 20,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 32\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 100,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.SmoothL1Loss(),\n",
    "        'num_epochs': 20,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 200,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    }\n",
    "]\n",
    "\n",
    "# Open the output file in append mode\n",
    "with open(\"two_fc_layer/book_results.txt\", \"a\") as output_file:\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        learning_rate = scenario['learning_rate']\n",
    "        loss_function = scenario['loss_function']\n",
    "        num_epochs = scenario['num_epochs']\n",
    "        scheduler = scenario['scheduler']\n",
    "        num_batches = scenario['num_batches']\n",
    "        optimizer_name = scenario['optimizer']\n",
    "        \n",
    "        # Create a new instance of the model and optimizer with the current hyperparameters\n",
    "        model = Recommender(num_users=len(lbl_user.classes_), num_isbns=len(lbl_book.classes_), embedding_dim=64)\n",
    "        if optimizer_name == 'SGD':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        # Create train and validation data loaders based on number of batches\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        \n",
    "        # Print the current scenario and number of batches\n",
    "        output_file.write(f\"Scenario {i+1}: Learning Rate = {learning_rate}, Loss Function = {loss_function.__class__.__name__}\\n\")\n",
    "        output_file.write(f\"Number of Batches: {num_batches}\\n\")\n",
    "        if scheduler is not None:\n",
    "            output_file.write(f\"Scheduler: {scheduler.__class__.__name__}\\n\")\n",
    "        output_file.write(\"Epoch\\tLoss\\tValidation loss\\tRMSE\\tValidation Accuracy\\n\")\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []  # List to store the training loss values\n",
    "        valid_losses = []  # List to store the validation RMSE values\n",
    "        valid_accuracies = []  # List to store the validation accuracies\n",
    "        rmse_values = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Switch to training mode\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch[\"user_id\"], batch[\"isbn\"])\n",
    "                loss = loss_function(outputs, batch[\"rating\"].unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step() \n",
    "            # Perform validation\n",
    "            model.eval()  # Switch to evaluation mode\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    user_ids, isbns, ratings = batch['user_id'], batch['isbn'], batch['rating']\n",
    "                    outputs = model(user_ids, isbns)\n",
    "                    ratings = ratings.view(-1, 1)  # Reshape the target tensor\n",
    "                    predicted_ratings = torch.round(outputs)  # Round the predicted ratings\n",
    "                    predictions.extend(predicted_ratings.tolist())\n",
    "                    targets.extend(ratings.tolist())\n",
    "                    correct = ((predicted_ratings == ratings) | (predicted_ratings == ratings - 0.5) | (predicted_ratings == ratings + 0.5)).sum().item()\n",
    "                    total_correct += correct\n",
    "                    total_samples += ratings.size(0)\n",
    "                    loss = loss_function(outputs, ratings)\n",
    "                    total_loss += loss.item()\n",
    "            valid_loss = total_loss / len(valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = mean_squared_error(targets, predictions, squared=True)\n",
    "            rmse_values.append(rmse)\n",
    "            # Calculate accuracy\n",
    "            accuracy = total_correct / total_samples\n",
    "\n",
    "            valid_accuracies.append(accuracy)\n",
    "\n",
    "            # Save the epoch results to the output file\n",
    "            output_file.write(f\"{epoch+1}\\t{epoch_loss:.4f}\\t{valid_loss:.4f}\\t{rmse:.2f}\\t{100*accuracy:.2f}\\n\")\n",
    "\n",
    "        output_file.write(\"\\n\")  # Add a separator between different scenarios\n",
    "\n",
    "        # Plot the training and validation losses\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(valid_losses, label='Validation loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss / RMSE')\n",
    "        plt.title(f'Training and Validation Loss - Scenario {i+1}')\n",
    "        plt.legend()\n",
    "        plot_file = os.path.join(\"two_fc_layer/Book_plots\", f'scenario_{i+1}_loss_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        # Plot the validation accuracy\n",
    "        plt.plot(valid_accuracies)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Validation Accuracy - Scenario {i+1}')\n",
    "        plot_file = os.path.join(\"two_fc_layer/Book_plots\", f'scenario_{i+1}_accuracy_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2198ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37cbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404ea82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2ea4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
