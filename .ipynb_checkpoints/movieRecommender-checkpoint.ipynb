{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a052dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model import Recommender\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from dataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0622e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "movie_ratings_df = pd.read_csv('dataset/MovieRatings.csv')\n",
    "movie_ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ace977f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         userId  movieId  rating   timestamp\n",
       "0            0        0     4.0   964982703\n",
       "1            0        2     4.0   964981247\n",
       "2            0        5     4.0   964982224\n",
       "3            0       43     5.0   964983815\n",
       "4            0       46     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     609     9416     4.0  1493848402\n",
       "100832     609     9443     5.0  1493850091\n",
       "100833     609     9444     5.0  1494273047\n",
       "100834     609     9445     5.0  1493846352\n",
       "100835     609     9485     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_book = preprocessing.LabelEncoder()\n",
    "movie_ratings_df['userId'] = lbl_user.fit_transform(movie_ratings_df['userId'].values)\n",
    "movie_ratings_df['movieId'] = lbl_book.fit_transform(movie_ratings_df['movieId'].values)\n",
    "\n",
    "user_ratings_count = Counter(movie_ratings_df['userId'])\n",
    "\n",
    "# Find users with less than 4 ratings\n",
    "users_to_remove = [user_id for user_id, count in user_ratings_count.items() if count < 4]\n",
    "\n",
    "# Remove users with less than 10 ratings from the dataset\n",
    "movie_ratings_df = movie_ratings_df[~movie_ratings_df['userId'].isin(users_to_remove)]\n",
    "movie_ratings_df.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e747d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(\n",
    "    movie_ratings_df, test_size=0.1, stratify=movie_ratings_df['rating'].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7309f58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10084, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066d8644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = MyDataset(train_df['userId'].values, train_df['movieId'].values, train_df['rating'].values)\n",
    "valid_dataset = MyDataset(valid_df['userId'].values, valid_df['movieId'].values, valid_df['rating'].values)\n",
    "\n",
    "# Create train and validation data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae03c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender(\n",
      "  (user_embedding): Embedding(610, 64)\n",
      "  (isbn_embedding): Embedding(9724, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Recommender(num_users=len(lbl_user.classes_),\n",
    "                        num_isbns=len(lbl_book.classes_),\n",
    "                        embedding_dim=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3fa1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler= optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "criterion = torch.nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13809424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# num_epochs = 5\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()  # Switch to training mode\n",
    "#     running_loss = 0.0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch[\"user_id\"], batch[\"isbn\"])\n",
    "#         loss = criterion(outputs, batch[\"rating\"].unsqueeze(1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "\n",
    "#     # Perform validation\n",
    "#     model.eval()  # Switch to evaluation mode\n",
    "#     total_loss = 0.0\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "#     predictions = []\n",
    "#     targets = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in valid_loader:\n",
    "#             user_ids, isbns, ratings = batch['user_id'], batch['isbn'], batch['rating']\n",
    "#             outputs = model(user_ids, isbns)\n",
    "#             ratings = ratings.view(-1, 1)  # Reshape the target tensor\n",
    "#             predicted_ratings = torch.round(outputs)  # Round the predicted ratings\n",
    "#             predictions.extend(predicted_ratings.tolist())\n",
    "#             targets.extend(ratings.tolist())\n",
    "#             correct = ((predicted_ratings == ratings) | (predicted_ratings == ratings - 0.5) | (predicted_ratings == ratings + 0.5)).sum().item()\n",
    "#             total_correct += correct\n",
    "#             total_samples += ratings.size(0)\n",
    "#             loss = criterion(outputs, ratings)\n",
    "#             total_loss += loss.item()\n",
    "#     valid_loss = total_loss / len(valid_loader)\n",
    "\n",
    "#     # Calculate RMSE\n",
    "#     rmse = mean_squared_error(targets, predictions, squared=True)\n",
    "\n",
    "#     # Calculate accuracy\n",
    "#     accuracy = total_correct / total_samples\n",
    "\n",
    "#     # Print the results for the current epoch\n",
    "#     print(f\"Epoch: {epoch+1}\")\n",
    "#     print(f\"Train Loss: {epoch_loss:.4f}\")\n",
    "#     print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "#     print(f\"RMSE: {rmse:.2f}\")\n",
    "#     print(f\"Validation Accuracy: {100*accuracy:.2f}%\")\n",
    "#     print()  # Print an empty line between epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21aae0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "C:\\Users\\Public\\Anaconda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "os.makedirs(\"four_fc_layer/Movie_plots\", exist_ok=True)\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        'optimizer': 'SGD',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 10,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 128\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 10,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 20,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 32\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.1,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 100,\n",
    "        'scheduler': None,\n",
    "        'num_batches': 64\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.SmoothL1Loss(),\n",
    "        'num_epochs': 20,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    },\n",
    "    {\n",
    "        'optimizer': 'Adam',\n",
    "        'learning_rate': 0.01,\n",
    "        'loss_function': torch.nn.MSELoss(),\n",
    "        'num_epochs': 200,\n",
    "        'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1),\n",
    "        'num_batches': 128\n",
    "    }\n",
    "]\n",
    "\n",
    "# Open the output file in append mode\n",
    "with open(\"four_fc_layer/movie_results.txt\", \"a\") as output_file:\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        learning_rate = scenario['learning_rate']\n",
    "        loss_function = scenario['loss_function']\n",
    "        num_epochs = scenario['num_epochs']\n",
    "        scheduler = scenario['scheduler']\n",
    "        num_batches = scenario['num_batches']\n",
    "        optimizer_name = scenario['optimizer']\n",
    "        \n",
    "        # Create a new instance of the model and optimizer with the current hyperparameters\n",
    "        model = Recommender(num_users=len(lbl_user.classes_), num_isbns=len(lbl_book.classes_), embedding_dim=64)\n",
    "        if optimizer_name == 'SGD':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        # Create train and validation data loaders based on number of batches\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        valid_loader = DataLoader(dataset=valid_dataset, batch_size=num_batches, shuffle=True, num_workers=4, drop_last=True)\n",
    "        \n",
    "        # Print the current scenario and number of batches\n",
    "        output_file.write(f\"Scenario {i+1}: Learning Rate = {learning_rate}, Loss Function = {loss_function.__class__.__name__}\\n\")\n",
    "        output_file.write(f\"Number of Batches: {num_batches}\\n\")\n",
    "        if scheduler is not None:\n",
    "            output_file.write(f\"Scheduler: {scheduler.__class__.__name__}\\n\")\n",
    "        output_file.write(\"Epoch\\tLoss\\tValidation loss\\tRMSE\\tValidation Accuracy\\n\")\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []  # List to store the training loss values\n",
    "        valid_losses = []  # List to store the validation RMSE values\n",
    "        valid_accuracies = []  # List to store the validation accuracies\n",
    "        rmse_values = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()  # Switch to training mode\n",
    "            running_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch[\"user_id\"], batch[\"isbn\"])\n",
    "                loss = loss_function(outputs, batch[\"rating\"].unsqueeze(1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(epoch_loss)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step() \n",
    "            # Perform validation\n",
    "            model.eval()  # Switch to evaluation mode\n",
    "            total_loss = 0.0\n",
    "            total_correct = 0\n",
    "            total_samples = 0\n",
    "            predictions = []\n",
    "            targets = []\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_loader:\n",
    "                    user_ids, isbns, ratings = batch['user_id'], batch['isbn'], batch['rating']\n",
    "                    outputs = model(user_ids, isbns)\n",
    "                    ratings = ratings.view(-1, 1)  # Reshape the target tensor\n",
    "                    predicted_ratings = torch.round(outputs)  # Round the predicted ratings\n",
    "                    predictions.extend(predicted_ratings.tolist())\n",
    "                    targets.extend(ratings.tolist())\n",
    "                    correct = ((predicted_ratings == ratings) | (predicted_ratings == ratings - 0.5) | (predicted_ratings == ratings + 0.5)).sum().item()\n",
    "                    total_correct += correct\n",
    "                    total_samples += ratings.size(0)\n",
    "                    loss = loss_function(outputs, ratings)\n",
    "                    total_loss += loss.item()\n",
    "            valid_loss = total_loss / len(valid_loader)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = mean_squared_error(targets, predictions, squared=True)\n",
    "            rmse_values.append(rmse)\n",
    "            # Calculate accuracy\n",
    "            accuracy = total_correct / total_samples\n",
    "\n",
    "            valid_accuracies.append(accuracy)\n",
    "\n",
    "            # Save the epoch results to the output file\n",
    "            output_file.write(f\"{epoch+1}\\t{epoch_loss:.4f}\\t{valid_loss:.4f}\\t{rmse:.2f}\\t{100*accuracy:.2f}\\n\")\n",
    "\n",
    "        output_file.write(\"\\n\")  # Add a separator between different scenarios\n",
    "\n",
    "        # Plot the training and validation losses\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(valid_losses, label='Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss / RMSE')\n",
    "        plt.title(f'Loss and RMSE - Scenario {i+1}')\n",
    "        plt.legend()\n",
    "        plot_file = os.path.join(\"four_fc_layer/Movie_plots\", f'scenario_{i+1}_loss_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "        # Plot the validation accuracy\n",
    "        plt.plot(valid_accuracies)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Validation Accuracy - Scenario {i+1}')\n",
    "        plot_file = os.path.join(\"four_fc_layer/Movie_plots\", f'scenario_{i+1}_accuracy_plot.png')\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62510a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
