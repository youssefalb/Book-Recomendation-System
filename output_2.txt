Scenario 1: Learning Rate = 0.1, Loss Function = MSELoss
Number of Batches: 64
Epoch	Loss	Validation loss	RMSE	Validation Accuracy
1	0.8006	0.7537	0.82	58.98
2	0.7331	0.7242	0.80	60.74
3	0.7061	0.7097	0.79	60.73
4	0.6887	0.6984	0.78	61.06
5	0.6772	0.6897	0.78	62.36
6	0.6683	0.6902	0.77	61.80
7	0.6614	0.6849	0.77	62.10
8	0.6562	0.6839	0.77	62.90
9	0.6501	0.6841	0.77	61.99
10	0.6453	0.6821	0.77	63.52

Scenario 2: Learning Rate = 0.1, Loss Function = MSELoss
Number of Batches: 32
Epoch	Loss	Validation loss	RMSE	Validation Accuracy
1	0.7877	0.7500	0.82	59.06
2	0.7290	0.7284	0.80	61.11
3	0.7057	0.7134	0.79	61.28
4	0.6912	0.7069	0.78	61.74
5	0.6815	0.6977	0.78	62.13
6	0.6739	0.6974	0.79	62.42
7	0.6677	0.6907	0.78	63.37
8	0.6633	0.6876	0.78	62.25
9	0.6586	0.7016	0.79	62.93
10	0.6551	0.6930	0.78	61.57
11	0.6516	0.6967	0.79	63.09
12	0.6491	0.6839	0.78	62.68
13	0.6456	0.6856	0.78	62.90
14	0.6437	0.7098	0.80	63.27
15	0.6410	0.6920	0.78	62.95
16	0.6383	0.6932	0.78	62.79
17	0.6360	0.6970	0.79	63.21
18	0.6336	0.6904	0.78	62.24
19	0.6305	0.6968	0.78	62.10
20	0.6292	0.7019	0.80	62.74

Scenario 3: Learning Rate = 0.1, Loss Function = MSELoss
Number of Batches: 64
Epoch	Loss	Validation loss	RMSE	Validation Accuracy
1	0.7986	0.7482	0.82	59.72
2	0.7292	0.7272	0.80	60.21
3	0.7018	0.7054	0.78	61.12
4	0.6855	0.7018	0.78	60.94
5	0.6749	0.6952	0.78	61.37
6	0.6665	0.6967	0.77	63.16
7	0.6601	0.6893	0.77	62.34
8	0.6546	0.6894	0.78	62.89
9	0.6504	0.6902	0.77	63.22
10	0.6461	0.6847	0.77	61.90
11	0.6429	0.6857	0.77	63.38
12	0.6396	0.6847	0.77	63.28
13	0.6362	0.6861	0.77	61.40
14	0.6335	0.6852	0.78	62.77
15	0.6317	0.6894	0.78	61.79
16	0.6295	0.6833	0.77	62.67
17	0.6279	0.6850	0.78	62.24
18	0.6258	0.6808	0.76	63.31
19	0.6234	0.6870	0.77	63.52
20	0.6218	0.6842	0.77	62.90
21	0.6197	0.6860	0.77	63.32
22	0.6187	0.6816	0.77	63.19
23	0.6165	0.6884	0.77	63.22
24	0.6155	0.6772	0.76	62.68
25	0.6135	0.6888	0.77	63.03
26	0.6112	0.6857	0.77	63.06
27	0.6101	0.6799	0.77	62.51
28	0.6087	0.6830	0.77	63.27
29	0.6070	0.6853	0.77	63.06
30	0.6056	0.6940	0.77	63.69
31	0.6044	0.6840	0.77	63.18
32	0.6039	0.6864	0.77	62.69
33	0.6019	0.6806	0.76	61.99
34	0.6009	0.6878	0.77	63.35
35	0.5991	0.6834	0.77	62.75
36	0.5983	0.6872	0.77	63.56
37	0.5970	0.6881	0.78	62.27
38	0.5956	0.6913	0.78	62.43
39	0.5939	0.6926	0.77	63.49
40	0.5931	0.6818	0.77	62.04
41	0.5913	0.6996	0.78	62.98
42	0.5903	0.6892	0.77	63.22
43	0.5897	0.6863	0.77	63.22
44	0.5885	0.6926	0.78	61.57
45	0.5869	0.6882	0.78	62.76
46	0.5853	0.6909	0.78	62.39
47	0.5844	0.6931	0.78	63.06
48	0.5835	0.6922	0.78	62.11
49	0.5823	0.6886	0.78	61.47
50	0.5806	0.6850	0.77	62.18
51	0.5799	0.6969	0.79	62.62
52	0.5786	0.6950	0.79	61.08
53	0.5770	0.6909	0.78	62.04
54	0.5766	0.6875	0.77	62.60
55	0.5751	0.6977	0.78	62.44
56	0.5744	0.7018	0.79	62.53
57	0.5724	0.7071	0.79	63.18
58	0.5716	0.6961	0.78	62.87
59	0.5705	0.7003	0.79	62.12
60	0.5696	0.7113	0.80	62.32
61	0.5682	0.7185	0.80	63.15
62	0.5669	0.7221	0.81	62.55
63	0.5661	0.7049	0.80	62.13
64	0.5649	0.7019	0.79	61.11
65	0.5638	0.6983	0.78	62.20
66	0.5628	0.7028	0.79	62.35
67	0.5616	0.7193	0.81	62.04
68	0.5603	0.6994	0.78	62.39
69	0.5593	0.7087	0.80	61.90
70	0.5582	0.7156	0.81	60.19
71	0.5569	0.7137	0.80	61.92
72	0.5557	0.7005	0.79	62.08
73	0.5544	0.7108	0.79	62.55
74	0.5536	0.7175	0.80	62.72
75	0.5528	0.7185	0.81	59.45
76	0.5515	0.7456	0.83	62.54
77	0.5507	0.7228	0.81	61.48
78	0.5494	0.7083	0.80	61.51
79	0.5476	0.7065	0.79	62.21
80	0.5467	0.7156	0.81	61.59
81	0.5454	0.7114	0.79	62.34
82	0.5447	0.7211	0.81	61.21
83	0.5435	0.7154	0.81	62.15
84	0.5423	0.7292	0.82	61.99
85	0.5412	0.7304	0.82	62.22
86	0.5399	0.7134	0.80	61.23
87	0.5383	0.7245	0.81	61.80
88	0.5374	0.7193	0.81	61.34
89	0.5363	0.7346	0.82	61.48
90	0.5353	0.7210	0.81	62.10
91	0.5343	0.7288	0.82	61.73
92	0.5332	0.7305	0.82	60.93
93	0.5319	0.7170	0.81	61.55
94	0.5306	0.7260	0.81	61.92
95	0.5299	0.7301	0.82	61.60
96	0.5282	0.7354	0.82	62.28
97	0.5271	0.7311	0.82	60.87
98	0.5261	0.7407	0.83	60.57
99	0.5249	0.7251	0.82	60.47
100	0.5237	0.7361	0.82	61.55

Scenario 4: Learning Rate = 0.01, Loss Function = SmoothL1Loss
Number of Batches: 128
Scheduler: StepLR
Epoch	Loss	Validation loss	RMSE	Validation Accuracy
1	0.4540	0.3680	0.87	57.08
2	0.3625	0.3602	0.85	57.60
3	0.3562	0.3558	0.85	57.89
4	0.3517	0.3522	0.84	58.16
5	0.3481	0.3489	0.84	58.57
6	0.3447	0.3461	0.83	58.78
7	0.3416	0.3434	0.83	59.06
8	0.3387	0.3412	0.83	59.13
9	0.3360	0.3396	0.83	59.00
10	0.3336	0.3369	0.83	59.39
11	0.3314	0.3353	0.83	59.42
12	0.3294	0.3336	0.83	59.49
13	0.3276	0.3321	0.83	59.60
14	0.3260	0.3313	0.83	59.72
15	0.3245	0.3301	0.83	59.51
16	0.3231	0.3289	0.83	59.62
17	0.3218	0.3278	0.83	59.94
18	0.3205	0.3271	0.82	59.93
19	0.3192	0.3257	0.82	60.19
20	0.3181	0.3249	0.82	60.06

Scenario 5: Learning Rate = 0.01, Loss Function = MSELoss
Number of Batches: 128
Scheduler: StepLR
Epoch	Loss	Validation loss	RMSE	Validation Accuracy
1	0.8968	0.8130	0.85	57.46
2	0.8006	0.7977	0.85	57.68
3	0.7856	0.7870	0.84	58.05
4	0.7739	0.7801	0.83	58.64
5	0.7641	0.7727	0.83	58.51
6	0.7560	0.7691	0.83	59.03
7	0.7490	0.7675	0.82	59.54
8	0.7428	0.7607	0.83	58.97
9	0.7374	0.7566	0.83	59.01
10	0.7324	0.7529	0.83	58.80
11	0.7276	0.7498	0.82	60.13
12	0.7231	0.7461	0.82	59.70
13	0.7193	0.7409	0.81	60.16
14	0.7153	0.7398	0.81	59.60
15	0.7117	0.7374	0.81	60.10
16	0.7081	0.7341	0.81	60.26
17	0.7043	0.7294	0.80	60.83
18	0.7005	0.7262	0.80	61.21
19	0.6970	0.7222	0.79	60.91
20	0.6935	0.7260	0.79	61.59
21	0.6898	0.7189	0.79	61.19
22	0.6865	0.7164	0.79	60.78
23	0.6831	0.7187	0.79	61.91
24	0.6803	0.7122	0.78	61.46
25	0.6769	0.7163	0.79	60.32
26	0.6743	0.7094	0.78	61.88
27	0.6714	0.7070	0.79	61.28
28	0.6685	0.7140	0.78	62.37
29	0.6662	0.7037	0.78	61.05
30	0.6638	0.7093	0.80	60.30
31	0.6617	0.7020	0.78	61.62
32	0.6593	0.7025	0.78	61.22
33	0.6574	0.7021	0.78	61.94
34	0.6552	0.7007	0.78	62.11
35	0.6530	0.6971	0.77	62.06
36	0.6513	0.6974	0.77	62.25
37	0.6492	0.6964	0.77	62.26
38	0.6476	0.6950	0.77	61.94
39	0.6461	0.6947	0.77	62.59
40	0.6448	0.6938	0.77	61.61
41	0.6432	0.6951	0.78	61.24
42	0.6417	0.6920	0.77	62.13
43	0.6403	0.7057	0.78	63.21
44	0.6391	0.6912	0.77	62.59
45	0.6376	0.6922	0.78	61.42
46	0.6363	0.6962	0.78	60.89
47	0.6351	0.6909	0.77	62.81
48	0.6341	0.6946	0.77	63.04
49	0.6333	0.6894	0.77	62.06
50	0.6318	0.6897	0.77	62.67
51	0.6310	0.6865	0.77	62.60
52	0.6300	0.6899	0.77	61.40
53	0.6295	0.6862	0.76	63.11
54	0.6284	0.6876	0.77	61.84
55	0.6275	0.6957	0.78	60.88
56	0.6269	0.6872	0.77	62.13
57	0.6258	0.6967	0.78	60.87
58	0.6257	0.6854	0.77	62.27
59	0.6247	0.6894	0.76	63.36
60	0.6241	0.6845	0.77	62.33
61	0.6233	0.6863	0.77	62.51
62	0.6225	0.6875	0.76	63.47
63	0.6221	0.6869	0.77	62.29
64	0.6218	0.6900	0.78	61.26
65	0.6209	0.6906	0.77	61.29
66	0.6202	0.6821	0.76	62.81
67	0.6194	0.6843	0.76	63.42
68	0.6191	0.6850	0.77	62.27
69	0.6186	0.6857	0.76	63.36
70	0.6181	0.6803	0.76	63.18
71	0.6176	0.6825	0.76	63.54
72	0.6168	0.6839	0.76	63.24
73	0.6168	0.6804	0.76	63.38
74	0.6162	0.6820	0.76	62.89
75	0.6151	0.6821	0.76	62.73
76	0.6150	0.6819	0.76	62.64
77	0.6147	0.6840	0.77	62.37
78	0.6142	0.6865	0.76	63.99
79	0.6139	0.6882	0.77	63.72
80	0.6135	0.6804	0.76	62.57
81	0.6127	0.6897	0.76	63.99
82	0.6127	0.6847	0.77	61.91
83	0.6120	0.6795	0.76	63.51
84	0.6117	0.6809	0.76	62.56
85	0.6113	0.6957	0.78	60.75
86	0.6109	0.6873	0.76	63.93
87	0.6107	0.6806	0.76	63.18
88	0.6100	0.6833	0.77	62.23
89	0.6096	0.6794	0.76	63.50
90	0.6092	0.6791	0.76	63.01
91	0.6089	0.6848	0.76	64.04
92	0.6086	0.6798	0.76	63.59
93	0.6083	0.6807	0.76	62.42
94	0.6079	0.6790	0.76	62.91
95	0.6073	0.6801	0.76	63.84
96	0.6073	0.6804	0.76	63.45
97	0.6069	0.6789	0.76	63.38
98	0.6065	0.6780	0.76	63.38
99	0.6063	0.6832	0.77	62.09
100	0.6061	0.6788	0.76	62.82
101	0.6055	0.6823	0.75	64.05
102	0.6050	0.6778	0.75	63.22
103	0.6047	0.6782	0.75	63.36
104	0.6045	0.6923	0.77	64.23
105	0.6042	0.6808	0.76	63.60
106	0.6041	0.6843	0.77	62.03
107	0.6037	0.6779	0.75	63.72
108	0.6037	0.6781	0.75	63.89
109	0.6032	0.6805	0.76	62.37
110	0.6027	0.6772	0.76	62.96
111	0.6025	0.6803	0.76	62.70
112	0.6023	0.6775	0.76	63.62
113	0.6017	0.6791	0.76	63.93
114	0.6014	0.6776	0.76	63.81
115	0.6014	0.6784	0.76	63.65
116	0.6011	0.6762	0.76	63.60
117	0.6008	0.6783	0.76	63.76
118	0.6005	0.6792	0.76	63.28
119	0.6004	0.6776	0.76	62.90
120	0.6000	0.6778	0.76	63.19
121	0.5996	0.6770	0.76	63.49
122	0.5995	0.6788	0.76	63.02
123	0.5991	0.6846	0.77	61.61
124	0.5992	0.6785	0.76	62.73
125	0.5986	0.6788	0.76	62.63
126	0.5983	0.6793	0.76	62.59
127	0.5978	0.6765	0.75	63.81
128	0.5977	0.6799	0.76	63.50
129	0.5975	0.6798	0.76	62.55
130	0.5975	0.6773	0.76	63.64
131	0.5969	0.6817	0.77	62.24
132	0.5968	0.6791	0.76	62.47
133	0.5964	0.6778	0.76	63.41
134	0.5965	0.6791	0.76	63.10
135	0.5963	0.6771	0.76	63.18
136	0.5956	0.6771	0.75	63.73
137	0.5957	0.6776	0.76	63.65
138	0.5956	0.6797	0.76	63.77
139	0.5951	0.6770	0.75	64.07
140	0.5952	0.6799	0.76	63.68
141	0.5948	0.6794	0.76	62.90
142	0.5943	0.6863	0.77	64.11
143	0.5945	0.6805	0.76	62.50
144	0.5940	0.6786	0.76	63.83
145	0.5938	0.6769	0.76	63.86
146	0.5936	0.6800	0.76	63.75
147	0.5937	0.6796	0.76	63.70
148	0.5934	0.6793	0.76	63.27
149	0.5932	0.6782	0.76	63.65
150	0.5929	0.6841	0.76	64.01
151	0.5927	0.6770	0.76	63.00
152	0.5924	0.6785	0.76	63.69
153	0.5921	0.6795	0.76	64.08
154	0.5919	0.6774	0.76	63.87
155	0.5918	0.6774	0.76	63.87
156	0.5916	0.6851	0.77	62.08
157	0.5914	0.6765	0.76	63.61
158	0.5909	0.6769	0.76	63.25
159	0.5911	0.6768	0.76	63.64
160	0.5905	0.6767	0.75	63.45
161	0.5906	0.6785	0.76	62.79
162	0.5903	0.6776	0.76	63.88
163	0.5901	0.6767	0.76	63.35
164	0.5901	0.6791	0.76	63.71
165	0.5897	0.6771	0.76	63.18
166	0.5897	0.6782	0.76	63.50
167	0.5895	0.6769	0.75	63.45
168	0.5895	0.6773	0.76	63.60
169	0.5889	0.6840	0.77	62.19
170	0.5888	0.6786	0.76	63.67
171	0.5888	0.6767	0.76	63.92
172	0.5887	0.6766	0.76	63.65
173	0.5884	0.6787	0.76	62.66
174	0.5879	0.6813	0.76	63.82
175	0.5882	0.6800	0.76	62.75
176	0.5879	0.6786	0.76	63.11
177	0.5875	0.6791	0.76	62.72
178	0.5871	0.6780	0.76	63.28
179	0.5871	0.6776	0.75	63.97
180	0.5868	0.6780	0.76	63.12
181	0.5868	0.6769	0.76	63.74
182	0.5865	0.6819	0.76	64.07
183	0.5865	0.6804	0.76	64.16
184	0.5862	0.6773	0.76	62.71
185	0.5861	0.6782	0.76	63.26
186	0.5859	0.6786	0.76	63.02
187	0.5860	0.6785	0.76	64.01
188	0.5857	0.6803	0.76	63.76
189	0.5856	0.6787	0.76	63.73
190	0.5852	0.6791	0.76	62.63
191	0.5851	0.6800	0.76	63.69
192	0.5849	0.6757	0.75	63.87
193	0.5846	0.6791	0.76	63.01
194	0.5844	0.6807	0.76	63.98
195	0.5844	0.6761	0.76	63.43
196	0.5844	0.6783	0.76	63.66
197	0.5841	0.6771	0.75	63.62
198	0.5839	0.6824	0.76	64.08
199	0.5836	0.6773	0.75	63.79
200	0.5835	0.6773	0.76	63.46

